标题: 未找到标题

文本内容:
3D Scene Transplantation: A Training-free Scene Construction Framework with Camera Trajectory Alignment and Lighting ConsistencyYuan Gao1, Jiaqi Xi1, Zishen Zhang1, Zhiyuan Zou1*Tingyuan Zhou2, and Jixuan Dai3 1 Beijing Information Science & Technology University, Beijing, China2 The 15th Institute of China Electronics Technology Group Corporation, Beijing, China3 Timber Chain Cloud (Suzhou) Digital Technology Co.,Ltd, Suzhou, China*zyzou@bistu.edu.cnAbstract. Computer-generated imagery plays a significant role in the modern multimedia industry, particularly in high-fidelity 3D object and scene reconstruction for movies and artwork. However, most commercial photography techniques rely on professional green-screen studios for foreground and background synthesis. This paper proposes a novel training-free 3D scene reconstruction framework based on 2D videos, referred to as 3D scene transplantation, for transplanting foreground objects into new scenes. Firstly, to solve the inconsistent lighting problem and avoid building a world-lighting model, we design a one-shot foreground and background processing pipeline using a natural language-driven image relighting method. Secondly, we propose a trajectory temporal-spatial matching method to address the foreground drifting problem in the new scene. Finally, a cross-view feature alignment method is proposed to further optimize the lighting in scene construction. Experimental results suggest that the proposed method can achieve photorealistic 3D reconstruction and deliver remarkable performance in foreground and background video synthesis.Keywords: computer generated imagery; 3D reconstruction; scene transplantation; image relightingIntroductionIn immersive content creation—such as film production, AR/VR environments, and digital human rendering—there is an increasing demand to seamlessly transplant a foreground subject into a new background scene [1-3] while maintaining both photorealistic appearance and spatial consistency. This process extends far beyond simple compositing: it requires the foreground subject to behave as though it physically exists within the new environment [4], accurately reflecting the target scene’s illumination, geometry, and camera motion.We define this challenge as 3D Scene Transplantation (3DST), a novel task aimed at reconstructing a high-fidelity 3D model of a foreground subject, which is relit and temporally aligned to match a reference background scene. Given a foreground video captured under arbitrary lighting conditions and a background video filmed with the same or a different camera trajectory [5], the goal is to synthesize a relit and temporally consistent video sequence that adheres to the lighting and spatial context of the new environment, enabling high-quality 3D reconstruction.This task presents several non-trivial challenges. First, the misalignment in trajectories between the foreground and background leads to noticeable foreground jitter, and, upon 3D reconstruction, the model exhibits scattered and distorted geometry. When foreground and background videos are recorded with different camera speeds or trajectories, ensuring spatiotemporal consistency between them becomes highly complex, often resulting in noticeable misalignment and a lack of coherence between the two. Second, without proper lighting adaptation, directly mapping the foreground onto the background leads to significant lighting inconsistencies between the two, resulting in a strong sense of incongruity. This problem becomes even more pronounced after 3D reconstruction, where mismatched lighting creates a jarring, unrealistic effect in the final scene.To address these challenges, we propose 3DST, a unified, view-aware framework designed to solve the 3DST task in both constrained and unconstrained capture scenarios. Our pipeline begins with a trajectory-matching module that aligns arbitrary foreground and background sequences in both pose and motion space, ensuring compatibility even when the videos are recorded with different camera speeds or trajectories. We then introduce UniLight, a latent-space neural relighting module that utilizes temporal transformers and view-aware graph regularization to achieve spatiotemporally consistent lighting adaptation across complex dynamic views. Finally, we optimize feature alignment to enhance both geometric and photometric fidelity under relit conditions, ensuring high-quality 3D reconstruction.Our main contributions are as follows:A unified relighting pipeline is proposed that leverages natural language-driven illumination guidance to achieve one-shot foreground–background harmonization, enabling temporally stable and spatially coherent lighting transfer without requiring explicit world lighting modeling.A trajectory synchronization strategy is developed to align camera pose, speed, and acceleration between unconstrained video pairs, effectively reducing foreground drift and ensuring consistent spatial-temporal integration.A cross-view feature alignment mechanism is introduced to enhance both appearance and geometric consistency across viewpoints, further improving the fidelity and realism of the reconstructed 3D scene under complex motion and lighting conditions.Related WorkForeground and Background IntegrationForeground–background integration plays a fundamental role in achieving photorealistic image and video compositing, particularly in dynamic environments where geometry, motion, and illumination are complex and constantly changing. Classical methods such as chroma keying and alpha blending perform reliably under controlled studio conditions [5], but degrade significantly in real-world scenes characterized by cluttered backgrounds and inconsistent lighting. To overcome these limitations, a growing body of work has explored learning-based harmonization techniques. Neural relighting models [6–8] have shown promise in adapting foregrounds to target lighting, either by disentangling intrinsic components such as albedo and shading [7], or via self-supervised representations [8] and diffusion-driven illumination alignment [6]. However, despite success on static, single-frame inputs, these models often fail to generalize to complex, spatially varying environments and lack the ability to model global illumination effects such as soft shadows and view-dependent reflectance. To further address spatial mismatch, several studies [9–13] have introduced background-aware or region-specific harmonization strategies. BargainNet [9] frames the task as a background-conditioned domain translation problem, while methods like FRIH [10] and HDNet [11] adopt hierarchical and region-aware architectures for fine-grained blending. Recent advances have incorporated globally guided feature transformation [12] and region-wise contrastive learning [13] to enhance semantic alignment and feature consistency. Meanwhile, temporal scenes bring new challenges including motion blur, lighting transitions, and cross-frame flicker. Multi-scale video segmentation frameworks [14] have been proposed to preserve temporal coherence in dynamic foregrounds, while ForAug [15] improves model robustness by recombining diverse foreground–background pairs. Though recent indoor relighting methods enable object insertion under complex lighting, they often fall short in maintaining global consistency across space and time, especially in the presence of occlusion, scene dynamics, and camera movement—challenges that are central to high-fidelity scene transplantation.Neural Rendering and 3D ReconstructionRecent advancements in neural rendering have substantially improved 3D reconstruction from sparse, multi-view images. Representative methods such as Neural Radiance Fields (NeRF) [16], Mip-NeRF [17], and 3D Gaussian Splatting (3DGS) [18] leverage implicit or hybrid representations to produce photorealistic and geometrically faithful reconstructions by optimizing appearance consistency across views. These techniques rely heavily on stable cross-view correspondences to recover fine-grained textures and detailed geometry. However, in real-world scenarios with uncontrolled lighting, inconsistencies such as illumination flicker and color drift are common, often disrupting feature matching and resulting in surface artifacts, blurred textures, and unstable reconstructions. To alleviate such issues, prior efforts have explored view-dependent regularization strategies [19] and per-view optimization frameworks [20], which aim to enforce appearance coherence across images. While these methods offer partial improvements, they tend to break down under large lighting variations due to their limited capacity to capture global illumination structure. To further improve multi-view consistency, a number of preprocessing strategies have been proposed, including tone mapping, deflickering, and histogram-based harmonization [21,22]. Though these approaches can suppress visual discontinuities to some extent, they are typically heuristic and operate independently of the reconstruction pipeline, lacking end-to-end adaptability to scene-level lighting dynamics. Moreover, they fall short in modeling long-range dependencies or adapting to complex spatial illumination patterns. Recent work highlights that robust 3D reconstruction under unconstrained lighting requires explicit handling of both global and local illumination effects [23]. Building on this insight, we introduce UniLight, a training-free neural relighting module that adaptively adjusts frame-wise illumination using priors extracted from the background. By enforcing spatiotemporal consistency before reconstruction, UniLight enhances the stability of geometry and appearance recovery under challenging lighting conditions, supporting high-fidelity 3D generation in dynamic scenes with unaligned trajectories and significant lighting disparity.Temporal and Trajectory AlignmentTemporal and trajectory alignment is essential for maintaining spatiotemporal coherence in video synthesis and 3D scene compositing. Discrepancies in camera trajectory, speed, or acceleration between foreground and background sequences often lead to artifacts such as foreground drift, motion jitter, and parallax mismatch, significantly compromising realism in transplanted scenes. Traditional approaches based on optical flow estimation [24], feature point matching [25], or SLAM techniques [26] tend to perform poorly in the presence of non-rigid motion, dynamic occlusion, or illumination variation. More recent learning-based methods, such as Temporal Alignment Networks (TAN) [27] and STAlignNet [28], model implicit motion fields or spatiotemporal correspondences and have shown improved performance on alignment and matting benchmarks. Transformer-based models [29,30] further enhance long-range temporal reasoning, enabling better handling of complex motion and view changes. However, many existing methods fail to account for photometric consistency—particularly lighting-aware motion cues such as soft shadow flow or shading variation across time—which can severely degrade results in lighting-sensitive neural rendering pipelines. Our framework addresses these challenges by introducing a trajectory matching module that explicitly synchronizes foreground and background sequences in both geometric and photometric domains, ensuring aligned camera motion and temporal consistency prior to reconstruction.MethodFig. 1. Overview of the 3DST framework: The proposed pipeline consists of three stages: (1) Trajectory Matching aligns the spatial and temporal motion of foreground and background videos; (2) UniLight performs temporally consistent relighting using a Transformer-based VAE; and (3) Feature-aligned 3D reconstruction produces a high-fidelity model from relit views.Problem FormulationWe are given a foreground video , a background video , and a textual prompt  describing the target illumination style, direction, or ambiance. Our goal is to learn a mapping function()that produces a relit video  and a high-fidelity 3D model . This mapping must satisfy three primary objectives. First, each frame  should exhibit lighting consistent with both  and the prompt . Second, consecutive frames  and  should maintain smooth illumination transitions and coherent geometry to avoid flicker or misalignment. Third, the resulting video  should preserve consistent illumination and structural detail across viewpoints so that methods such as 3D Gaussian Splatting can reconstruct  with accurate geometry and appearance.Under trajectory-aligned captures of foreground and background scenes, our method leverages illumination priors encoded in  and  to guide frame-wise relighting with spatiotemporal consistency. The final relit video  is then employed for 3D reconstruction, yielding a stable and detail-preserving model .Trajectory Matching for Arbitrary Scene PairsMany real-world applications involve foreground and background videos recorded with different camera trajectories. To address these unconstrained conditions, we introduce a trajectory matching strategy that aligns arbitrary foreground and background videos in both spatial and temporal dimensions. This approach ensures geometric consistency and motion similarity, which serves as a foundation for coherent relighting and 3D reconstruction.Fig. 2. Trajectory Matching Pipeline with Spatial and Temporal Alignment3D Trajectory Estimation. We begin by estimating camera trajectories for both foreground and background using Structure-from-Motion (SfM) via COLMAP. This process recovers a sequence of extrinsic parameters for each video:(2)To reduce noise and facilitate smooth motion analysis, we fit each discrete trajectory with a B-spline in time. This produces continuous representations of camera translation  and its first and second derivatives  and .Spatial Trajectory Alignment. Foreground and background cameras may reside in different coordinate frames due to varied initial positions or setups. We perform a rigid similarity transform (rotation , translation , and scale ) that aligns the two trajectories by minimizing:(3)The Umeyama algorithm provides an initial solution, followed by ICP refinement to handle outliers or minor drift. Once this optimization converges, each foreground point  is mapped to(4)Temporal Synchronization and Motion Matching. We next seek a time-warping function  that aligns the motion of both sequences. The matching objective combines geometric and kinematic terms:(5)Here  and . The function  is initially approximated via Dynamic Time Warping and refined using gradient-based optimization, resulting in aligned subsequences  and  that are synchronized in both space and motion.Robustness Enhancements. To improve stability, we remove frames with noisy camera poses, discard pairs that exhibit occlusions or poor view overlap, enforce monotonicity in , and apply confidence weighting based on visual similarity or trajectory curvature. These safeguards help maintain consistent alignment under occlusions or subtle drift.After trajectory matching, we obtain two sequences  and  that share similar poses, exhibit closely matched velocities, and are resilient to offset or jitter. The following sections describe how these aligned sequences guide spatiotemporal relighting (Sec. 3.3) and facilitate consistent 3D reconstruction (Sec. 3.4).UniLight: Latent-Space Video Relighting with View-Aware Temporal ConsistencyAlthough the alignment in Sec. 3.2 places foreground and background videos in comparable trajectories, frame-wise relighting models (e.g., IC-Light or diffusion-based methods) still treat each frame independently. This isolation leads to visible flicker or shadow mismatch when the camera captures continuous orbital paths around the subject. We address these limitations by introducing UniLight, a latent-space video relighting framework that enforces temporal continuity and accounts for view-dependent lighting.Fig. 3. UniLight: Temporally Consistent Neural Relighting Framework for Scene AdaptationLatent-Space Relighting with Per-Frame Adaptation. Let  be the aligned foreground frames. Each frame is processed by a pretrained single-frame relighting model , producing(6)where  specifies the target illumination inferred from the background video or a style prompt. The resulting frames  are then projected into a shared VAE latent space. Concretely,(7)where  is a compact feature representation that captures semantic and appearance aspects of each relit frame.Temporal Transformer for Global Consistency. To reconcile illumination inconsistencies across time, we employ a temporal transformer . This module ingests the sequence  and outputs(8)Each  is tokenized with temporal position encodings. The transformer attends globally to all frames, unifying lighting across the sequence while respecting natural view-dependent variations caused by parallax or changing material response. This approach ensures consistency without suppressing genuine lighting changes from different viewpoints.FCG-Net: View-Aware Graph-Based Global Regularization. Although the temporal transformer reduces most local and mid-range flicker, wide-angle or long-sequence drift may still persist. To address this issue, we introduce FCG-Net, a lightweight Frame-Consistency Graph Network that models the entire sequence as a temporal graph . Each node in  corresponds to a latent frame , and edges in  carry weights reflecting similarity and camera-pose proximity. Specifically,(9)where  computes an average feature, and  encodes geometric adjacency of poses  and . A shallow GNN then extracts a global illumination context . Each frame refines its latent representation via FiLM modulation:(10)which draws on holistic sequence information to stabilize illumination.Output Synthesis. The refined latents  are passed to the VAE decoder, yielding final relit frames(11)Training is supervised by a weighted combination of perceptual fidelity, temporal consistency, and global style alignment:(12)where  ensures fidelity to the single-frame result,  penalizes discontinuities between adjacent frames, and  enforces scene-wide illumination matching to the background. UniLight thus acts as a global, non-iterative refinement stage that complements single-frame relighting by producing flicker-free, view-aware outputs. These relit frames serve as a more stable input for 3D reconstruction, described next.Point Cloud Feature AlignmentOnce UniLight has produced a flicker-free sequence with uniform illumination, multi-view geometry becomes more reliable. This consistency is further refined by addressing residual appearance or geometric mismatches in three dimensions, enhancing the final reconstructed model’s fidelity.Fig. 4. Feature-Aligned Point Cloud Optimization for High-Fidelity 3D ReconstructionInitial Structure-from-Motion and Sparse Point Cloud. We first run COLMAP’s sparse reconstruction pipeline on the relit frames  to obtain camera poses  and a sparse point cloud . The standard Structure-from-Motion (SfM) process includes feature detection, matching, and outlier filtering. By using the temporally stable illumination generated by UniLight, we significantly reduce mismatches caused by shading inconsistencies or exposure shifts, leading to a cleaner initial reconstruction.Point Cloud Feature Alignment. After obtaining the initial sparse point cloud from COLMAP, we integrate the feature alignment directly within the SfM pipeline to enhance the point cloud accuracy. Specifically, we use feature alignment to refine the positions and appearances of the sparse points PPP across multiple views. This is done by aligning the features extracted from the initial sparse point cloud with the relit frames, ensuring consistent spatial and appearance-based properties across different views.To achieve this, we introduce a feature alignment loss during the SfM phase. The alignment is performed by minimizing the difference in extracted features between matching views. The feature alignment loss is formulated as:(13)where  represents a set of view pairs for alignment, and  is a feature extraction function based on color histograms and CNN-based embeddings. This loss function encourages the sparse point ​ to maintain consistent appearance and spatial location across different viewpoints. The parameters of each point, including position and appearance, are then optimized jointly during the SfM process, improving the initial point cloud’s accuracy and coherence before further 3D reconstruction steps.By introducing feature alignment in the COLMAP stage, we ensure a more consistent and accurate point cloud for subsequent stages of the 3D reconstruction pipeline. This results in a high-fidelity 3D representation, setting a strong foundation for later refinement with methods like 3D Gaussian Splatting. The following section provides quantitative and qualitative evaluations across diverse scenes.ExperimentsExperimental SetupTo evaluate the generalizability and efficiency of our 3DST framework in diverse scenarios, we construct a custom dataset comprising 42 foreground instances and 78 background environments. The foregrounds include plush dolls, plastic materials, toy vehicles, human figures, and synthetic characters, while the backgrounds span a wide range of indoor and outdoor scenes with varied lighting, including both real-world captures and high-fidelity synthetic renderings created in Cinema 4D. Each foreground–background pair is independently acquired or generated, enabling flexible, unconstrained combinations across scenes and lighting conditions.Fig. 5. Qualitative Results of 3DST on Diverse Foreground–Background CombinationsThe pipeline is evaluated on a single NVIDIA RTX 4090 GPU with 24GB VRAM. For each experiment, 90-frame sequences are processed within approximately one hour, including both relighting and reconstruction. Each relit frame undergoes 30 inference steps using our UniLight module, and the resulting frames are subsequently passed to 3D reconstruction backends. By default, we use 3D Gaussian Splatting (GauSplat) for reconstruction due to its fast convergence and superior fidelity. Additionally, to demonstrate the compatibility of our pipeline with other neural rendering approaches, we conduct comparative experiments using Instant-NGP (a fast implementation of NeRF).Fig. 5 showcases qualitative results on five representative foreground–background pairs, demonstrating the model’s ability to adapt to varying shapes, materials, and illumination conditions. Table 1 provides detailed runtime statistics and resource usage for each scene. Results show that our pipeline scales well with resolution and sequence length, with peak VRAM usage remaining below 23 GB and processing time ranging from 52 to 103 minutes depending on the resolution and frame count. These experiments validate the robustness, scalability, and adaptability of our framework across heterogeneous inputs and reconstruction backends.Table 1. Runtime and resource consumption of 3DST on various foreground–background video pairsForeground andBackgroundFrame CountResolutionRelighting Time(per frame)Total Time Peak VRAMUsageRabbit+grassland90720p10.4s52 min18.2 GBCar+road901080p15.2s72 min21.4 GBEgg+library300720p11.2s86 min19.6 GBBear+grassland3001080p15.6s103 min22.1 GBBird+grassland2901080p14.8s75 min20.7 GBWoman_room901080p16.4s79min21.9GBComposite scene1201080p17.2s98min22.1GBEvaluationTo comprehensively validate the effectiveness of our 3DST framework, we conduct a series of qualitative and quantitative comparisons across key dimensions of the scene transplantation pipeline: trajectory alignment, illumination adaptation, and reconstruction fidelity. Our experiments focus on analyzing the impact of each module and assessing the robustness of the system under challenging real-world capture conditions. Unless otherwise noted, all results are generated from our real-captured dataset, and visualizations are shown in both 2D compositing and 3D reconstruction views.Fig. 6. 2D compositing results under different alignment settings. Without trajectory alignment, the foreground exhibits severe temporal jitter and spatial drift. Our method produces stable and temporally coherent results across frames.Fig. 7. 3D reconstruction comparison with and without trajectory alignment. Without alignment, severe ghosting and geometry splitting occur due to inconsistent viewpoints. Our method ensures stable and coherent reconstruction.Trajectory Alignment Evaluation. To investigate the impact of trajectory misalignment, we begin by comparing unaligned and aligned foreground–background sequences. As shown in Fig. 6, misaligned inputs introduce severe temporal jitter and spatial drift in the 2D composites, causing the foreground object to shift inconsistently across frames. This not only undermines visual realism but also disrupts motion continuity, making the subject appear detached from the background. In contrast, our trajectory matching module enforces spatiotemporal coherence by synchronizing camera pose, velocity, and acceleration, producing significantly more stable and visually plausible results.The downstream impact of misalignment becomes even more pronounced in the 3D domain. As illustrated in Fig. 7, reconstructions from unaligned sequences exhibit ghosting artifacts and geometry duplication due to inconsistent parallax and camera viewpoints. Without trajectory synchronization, the 3D model fragments under view aggregation. Our method, by enforcing accurate trajectory alignment, yields a structurally stable and coherent mesh with clean surfaces and consistent geometry. This experiment underscores the importance of motion alignment as a prerequisite for both high-fidelity rendering and reliable 3D reconstruction.Fig. 8. 2D compositing under strong background lighting. Without relighting, the foreground object appears visually detached due to lighting mismatch. Our method adapts the foreground to ambient green illumination, achieving harmonious integration.Fig. 9. 3D reconstruction under ambient lighting mismatch. Direct fusion causes photometric inconsistency and degraded geometry due to lighting conflict. Our illumination adaptation leads to visually coherent and structurally stable reconstructions.Illumination Adaptation Evaluation. To assess the impact of lighting inconsistency, we compare direct foreground–background fusion with and without relighting. As illustrated in Fig. 8, directly compositing the foreground onto a background scene with dominant ambient illumination—such as saturated green lighting—results in noticeable photometric disparity. The unrelit foreground retains its original lighting cues, producing unnatural highlights and shadows that starkly contrast with the target environment. This mismatch causes the inserted subject to appear visually detached, undermining realism and perceptual coherence.The negative effects of this inconsistency extend into the 3D reconstruction stage. As shown in Fig. 9, directly fused sequences introduce severe appearance discontinuities across views, which disrupt feature correspondence and ultimately degrade reconstruction quality. The resulting geometry is fragmented, and the textures exhibit instability and distortion. In contrast, applying our UniLight module prior to reconstruction harmonizes lighting across frames by adapting the foreground appearance to the ambient context of the background. This leads to view-consistent inputs that enhance both photometric and geometric fidelity. The final 3D models exhibit improved structural integrity and visual coherence, demonstrating the critical role of relighting in enabling stable and realistic scene transplantation.Table 2. Quantitative comparison of 3DST performance under different backbonesMethodIterationsPSNR ↑SSIM ↑LPIPS ↓Training Time (min)NeRF(Instant-NGP)3000024.30.9100.12213min45sGauSplat3000026.80.9330.08119min25sFig. 10. Comparison of 3D reconstruction backends integrated with 3DST.Both GauSplat and NeRF benefit from our unified relighting and alignment pipeline. GauSplat yields sharper textures, while NeRF demonstrates slightly more structural blur in dynamic regions.3D Reconstruction Method Comparison. We evaluate the effectiveness of our unified pipeline when paired with two mainstream reconstruction backbones: NeRF (Instant-NGP) and 3D Gaussian Splatting (GauSplat). As depicted in Fig. 10, both backends operate on the same relit and trajectory-aligned inputs to ensure a fair and consistent comparison. GauSplat produces cleaner geometry and sharper textures across views, exhibiting stronger spatial coherence and fewer artifacts. NeRF, while capable of capturing high-frequency details, tends to introduce structural blur in dynamic regions, largely due to its limited temporal consistency and slower convergence.The quantitative results in Table 2 further support these observations. GauSplat outperforms NeRF across all evaluation metrics—achieving higher PSNR (26.8 vs. 24.3), better structural similarity (SSIM 0.933 vs. 0.910), and lower perceptual error (LPIPS 0.081 vs. 0.122). Although NeRF completes training faster (13min45s vs. 19min25s), it falls short in visual fidelity under complex lighting and motion. These findings affirm that our relighting and alignment framework generalizes well across diverse reconstruction pipelines, consistently enhancing output quality.Fig. 11. Reconstruction quality of 3DST under different training iterations. With longer training, 3DST yields improved lighting fidelity, sharper geometry, and more stable integration.Reconstruction Fidelity vs. Training Iterations. We examine the impact of training duration by comparing GauSplat results at 7k, 15k, and 30k iterations. As illustrated in Fig. 11, shorter training yields coarse geometry and blurred textures, while longer optimization significantly enhances lighting fidelity, structural detail, and visual stability. These results highlight the trade-off between computational cost and reconstruction quality, offering practical guidance for resource-constrained deployment.Ablation StudyWe conduct a series of ablation experiments to examine the individual contribution of the three main modules in 3DST: trajectory synchronization, illumination adaptation via UniLight, and cross-view feature alignment. For each variant, we remove one component while keeping the rest of the pipeline unchanged. The visual comparisons are shown in Fig. 12, and the corresponding quantitative results are reported in Table 3.Fig. 12. Ablation Study on the Effectiveness of Trajectory Alignment, Relighting, and Feature Consistency in 3DSTRemoving the trajectory synchronization module causes noticeable temporal jitter and spatial misalignment between the foreground and background sequences. In the rendered video, the foreground object exhibits unstable motion, and the reconstructed 3D geometry suffers from ghosting and fragmentation due to inconsistent viewpoints. This degradation is reflected in significantly lower evaluation scores, with PSNR dropping to 20.9 and LPIPS increasing to 0.198, indicating that trajectory alignment is essential for both visual coherence and geometric integrity.When UniLight is excluded, the foreground is directly composited into the background without any relighting. This leads to evident shading mismatches and unnatural illumination, especially under high-contrast lighting conditions. Although the geometric structure remains relatively consistent, the photometric disparity disrupts feature correspondences during reconstruction, resulting in less stable textures and reduced realism. The decline in PSNR and SSIM, along with the increase in LPIPS, illustrates the critical role of illumination adaptation in achieving photometric consistency.Finally, removing the feature alignment module yields reconstructions with generally preserved geometry but weakened appearance consistency across views. In particular, the relit frames tend to exhibit subtle color shifts, local shading discrepancies, and reduced surface fidelity. While the overall structure is not severely impacted, the lighting appears less coherent and lacks the visual harmony present in the full model. This observation is supported by moderate drops in SSIM and LPIPS, confirming that feature alignment not only stabilizes spatial correspondence but also refines lighting integration across perspectives.Together, these results demonstrate that each component plays a distinct and complementary role in the pipeline. Trajectory alignment ensures motion-level coherence, UniLight resolves photometric inconsistency, and feature alignment enhances multi-view realism by further improving lighting fidelity and appearance consistency. Their joint design is key to the robustness and high-quality reconstruction performance of 3DST.Table 3. Ablation Study: Module Contribution AnalysisSettingPSNR ↑SSIM ↑LPIPS ↓Full 3DST26.80.9330.081w/o Trajectory Matching20.90.8210.198w/o Relighting (UniLight)23.70.8760.143w/o Feature Alignment24.10.8840.139Conclusion and DiscussionThis paper presents 3D Scene Transplantation (3DST), a training-free framework for seamlessly transplanting dynamic foreground objects into new backgrounds while preserving photorealistic appearance, spatial coherence, and temporal consistency. Unlike prior approaches that depend on pre-aligned data or studio setups, 3DST handles unconstrained foreground–background video pairs with varying camera trajectories and lighting conditions.The proposed pipeline integrates three core components: trajectory synchronization to align motion and camera geometry, UniLight for view-consistent neural relighting, and cross-view feature alignment for enhanced appearance and structure consistency. Extensive experiments show that each module is essential. Trajectory alignment reduces foreground drift and improves geometric stability, UniLight mitigates illumination mismatch, and feature alignment refines multi-view coherence. Together, these components enable high-fidelity 3D reconstruction and visually realistic scene integration, surpassing existing baselines across diverse scenarios.Despite its strong performance, the current system has limitations. It assumes reasonably accurate foreground segmentation, which may be challenged in cluttered or occluded scenes. Additionally, the computational demands—particularly for long or high-resolution sequences—could be further optimized using hierarchical or lightweight architectures. Future extensions could also explore interactive scene editing and real-time feedback, enhancing the framework’s usability in AR/VR content creation and video production.Overall, 3DST demonstrates a promising step toward generalizable, controllable, and high-quality 3D video synthesis in real-world environments by bridging trajectory-aware alignment, neural relighting, and robust reconstruction.ReferencesPires, F., Silva, R., Raposo, R.: A survey on virtual production and the future of compositing technologies. Avanca Cinema Journal 21, 692–699 (2022)Rekik, R., Wuhrer, S., Hoyet, L., Zibrek, K., Olivier, A.-H.: A survey on realistic virtual human animations: definitions, features and evaluations. Comput. Graph. Forum 43(2), e15064 (2024)Korkut, E.H., Surer, E.: Visualization in virtual reality: a systematic review. Virtual Reality 27(2), 1447–1480 (2023)Yuan, B., Peng, S., Wang, L., Ceylan, D., Bickel, B., Pollefeys, M., Tang, D.: NeRF-Editing: Geometry Editing of Neural Radiance Fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 18353–18363 (2022)Gojcic, Z., Zhou, C., Wegner, J.D., Wieser, A.: Deep Global Features for Point Cloud Alignment. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 746–763. Springer, Cham (2020)Garcia-Peraza-Herrera, L.C., Li, W., Clarkson, M.J., et al.: Image compositing for segmentation of surgical tools without manual annotations. IEEE Trans. Med. Imaging 40(5), 1450–1460 (2021)Zhang, L., Rao, A., Agrawala, M.: Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In: Proc. Int. Conf. on Learning Representations (ICLR) (2025)Jiang, Y.F., Gong, X., Liu, D., et al.: EnlightenGAN: Deep light enhancement without paired supervision. IEEE Trans. Image Process. 30, 2340–2349 (2021)Liu, Y., Chen, X., Xie, J., et al.: Relighting images in the wild with a self-supervised Siamese auto-encoder. In: Proc. IEEE/CVF Winter Conf. on Applications of Computer Vision (WACV), pp. 161–170 (2021)Yang, Z.X., Wei, Y.C., Yang, Y.: Collaborative video object segmentation by multi-scale foreground-background integration. IEEE Trans. Pattern Anal. Mach. Intell. 44(9), 4701–4712 (2021)Cong, W., Niu, L., Zhang, J., et al.: BargainNet: Background-guided domain translation for image harmonization. In: Proc. IEEE Int. Conf. on Multimedia and Expo (ICME), pp. 1–6 (2021)Peng, J., Luo, Z., Liu, L., et al.: FRIH: Fine-grained region-aware image harmonization. arXiv preprint arXiv:2205.06448 (2022)Chen, H., Gu, Z., Li, Y., et al.: Hierarchical dynamic image harmonization. arXiv preprint arXiv:2211.08639 (2022)Niu, L., Tan, L., Tao, X., et al.: Deep image harmonization with globally guided feature transformation and relation distillation. arXiv preprint arXiv:2308.00356 (2023)ForAug: Recombining foregrounds and backgrounds to improve robustness. arXiv preprint arXiv:2503.09399 (2025)Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In: Proc. European Conf. on Computer Vision (ECCV), pp. 405–421 (2020)Zhang, B., Jiang, C., Snavely, N., Hedman, P.: Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In: Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 5855–5864 (2022)Qian, K., Kopanas, G., Tretschk, E., et al.: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. (SIGGRAPH) 43(4), Article 1–14 (2024)Liu, L., Lin, T., Li, Z., et al.: Neural Sparse Voxel Fields with View-Dependent Appearance Regularization. In: Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 14609–14618 (2021)Lin, C., Ma, P., Tang, K., et al.: Robustifying NeRF with Per-View Optimization. In: Proc. European Conf. on Computer Vision (ECCV), pp. 265–282 (2022)Wu, Z., Wu, J., Wang, Q., et al.: Illumination Harmonization for Multi-view Neural Rendering. In: Proc. IEEE/CVF Int. Conf. on Computer Vision (ICCV), pp. 13010–13020 (2023)Zhao, Y., Liao, J., Chen, Y., et al.: Deflicker-NeRF: Consistent Radiance Field Reconstruction from Flickering Videos. In: Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 14893–14902 (2024)Jiang, Q., Gao, L., Tang, P., et al.: Multi-Agent Reinforcement Learning for Traffic Signal Control through Universal Communication Method. arXiv preprint arXiv:2204.12190 (2022)Sun, D., Yang, X., Liu, M.Y., Kautz, J.: PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8934–8943 (2018)Rocco, I., Arandjelović, R., Sivic, J.: End-to-end weakly-supervised semantic alignment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6917–6925 (2018)Mur-Artal, R., Tardós, J.D.: ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras. IEEE Trans. Robot. 33(5), 1255–1262 (2017)Han, T., Xie, W., Zisserman, A.: Temporal alignment networks for long-term video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3170–3180 (2022)Wang, Z., Zhang, Q., Zhu, Y., Dai, Q.: STAlignNet: Spatio-temporal alignment for foreground-background video matting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6646–6655 (2021)Bertasius, G., Wang, H., Torresani, L., et al.: Space-Time Attention for Video Understanding. In: Proceedings of the 38th International Conference on Machine Learning (ICML), pp. 884–895 (2021)Diba, A., Sharma, V., Arzani, M., Gool, L.V.: Spatio-temporal convolution-attention video network. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9278–9287 (2023)